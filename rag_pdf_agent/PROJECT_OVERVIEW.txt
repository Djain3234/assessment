"""
COMPLETE PROJECT OVERVIEW
=========================

This is a production-ready RAG (Retrieval-Augmented Generation) system
that provides strictly grounded conversational Q&A over PDF documents.

KEY IMPLEMENTATION DETAILS
---------------------------

1. ARCHITECTURE
   - Modular design with clear separation of concerns
   - Pipeline: Ingest → Index → Retrieve → Generate
   - Stateful conversation with history management

2. CORE COMPONENTS

   a) ingest.py - PDF Processing
      - Uses pypdf for text extraction
      - Page-by-page extraction with metadata
      - Overlapping chunking strategy
      - Preserves page numbers and chunk IDs

   b) retriever.py - Vector Search
      - sentence-transformers for embeddings
      - FAISS for efficient similarity search
      - Cosine similarity scoring
      - Index persistence for reuse

   c) prompt.py - Grounded Prompting
      - System instructions enforce grounding
      - Dynamic prompt construction
      - Query rewriting for follow-ups
      - Citation format enforcement

   d) chat.py - Conversational Agent
      - Google Gemini API integration
      - Multi-turn conversation support
      - History-aware query rewriting
      - Debug mode for transparency

   e) main.py - CLI Entry Point
      - Argument parsing
      - End-to-end orchestration
      - Index caching support
      - Error handling

3. GROUNDING STRATEGY

   The system enforces strict document grounding through:
   
   a) Retrieval-first approach
      - Always retrieves relevant chunks before answering
      - No direct question-answering without retrieval
   
   b) Explicit system instructions
      - Clear rules in prompt about using only retrieved content
      - Instruction to return "Not found" if answer not in chunks
   
   c) Citation requirements
      - Forces model to cite sources
      - Citations tied to specific chunks and pages
   
   d) Evidence sections
      - Requires quoting from retrieved text
      - Makes hallucination visible

4. CONVERSATION HANDLING

   Multi-turn conversations work through:
   
   a) History preservation
      - Stores (user_query, assistant_response) tuples
      - Limited to last 2-3 turns to avoid context overflow
   
   b) Query rewriting
      - Detects follow-up questions (pronouns, short queries)
      - Rewrites to standalone questions using Gemini
      - Uses rewritten query for retrieval
   
   c) History injection
      - Includes recent turns in generation prompt
      - History used ONLY for context, not facts
      - Retrieved chunks remain the source of truth

5. RETRIEVAL MECHANISM

   Vector search implementation:
   
   a) Embedding generation
      - all-MiniLM-L6-v2 model (384 dimensions)
      - Fast and accurate for English text
      - Normalized for cosine similarity
   
   b) FAISS indexing
      - IndexFlatIP for inner product (cosine similarity)
      - Exact search (no approximation)
      - Supports up to millions of vectors
   
   c) Top-k retrieval
      - Configurable (default: 5 chunks)
      - Returns chunks with similarity scores
      - Chunks ordered by relevance

6. RESPONSE GENERATION

   Gemini API integration:
   
   a) Model selection
      - gemini-1.5-flash: Fast, cost-effective
      - gemini-1.5-pro: More capable, higher quality
   
   b) Prompt structure
      - System instruction (grounding rules)
      - Retrieved chunks with citations
      - Conversation history (if any)
      - Current user question
   
   c) Response format
      - Short answer
      - Citations [p{page}:c{chunk}]
      - Evidence section with quotes

7. ERROR HANDLING

   The system handles:
   - Missing PDF files
   - Extraction failures
   - Empty documents
   - API errors
   - Invalid API keys
   - Index corruption

8. PERFORMANCE OPTIMIZATIONS

   a) Index caching
      - Saves FAISS index to disk
      - Saves chunks with pickle
      - Reuse for subsequent runs
   
   b) Batch operations
      - Batch embedding generation
      - Progress bars for long operations
   
   c) Efficient chunking
      - Overlapping chunks for context
      - Configurable chunk size
      - Balance between granularity and performance

9. TESTING APPROACH

   Test with:
   - Different document types (earnings, contracts, reports)
   - Various question types (factual, summarization, comparison)
   - Follow-up questions
   - Questions with no answer in document
   - Multi-step reasoning questions

10. LIMITATIONS AND CONSIDERATIONS

    a) PDF limitations
       - Text-based PDFs only (no OCR)
       - Table extraction may be imperfect
       - Images not processed
    
    b) Retrieval limitations
       - Depends on chunk quality
       - May miss information split across chunks
       - Semantic search may miss exact keywords
    
    c) Generation limitations
       - Depends on model capabilities
       - May struggle with complex reasoning
       - Citation format depends on model compliance
    
    d) Performance considerations
       - First run slower (embedding generation)
       - Memory usage scales with document size
       - API rate limits may apply

11. EXTENSIBILITY

    Easy to extend:
    - Add different embedding models
    - Swap FAISS for other vector DBs (Chroma, Pinecone)
    - Use different LLMs (Claude, OpenAI)
    - Add OCR preprocessing
    - Implement hybrid search (keyword + semantic)
    - Add re-ranking stage
    - Implement streaming responses

12. PRODUCTION CONSIDERATIONS

    For production deployment:
    - Add rate limiting
    - Implement request logging
    - Add monitoring and metrics
    - Handle concurrent requests
    - Implement authentication
    - Add result caching
    - Set up error alerting
    - Add input validation
    - Implement output filtering

COMMAND-LINE INTERFACE
----------------------

Usage: python main.py <pdf_path> [options]

Required:
  pdf_path              Path to PDF document

Optional:
  --model MODEL         Gemini model (default: gemini-1.5-flash)
  --top-k K             Number of chunks to retrieve (default: 5)
  --chunk-size SIZE     Chunk size in characters (default: 800)
  --chunk-overlap N     Overlap in characters (default: 200)
  --no-debug            Disable retrieval debug output
  --use-cache           Use cached index if available

Environment:
  GEMINI_API_KEY        Google Gemini API key (required)

PROGRAMMATIC API
----------------

See example_usage.py for examples of:
- Creating ingestion pipeline
- Building custom retrievers
- Configuring chat agents
- Batch question processing
- Custom error handling

FILES OVERVIEW
--------------

Core modules:
- main.py (163 lines): CLI and orchestration
- ingest.py (119 lines): PDF extraction and chunking
- retriever.py (172 lines): Vector search with FAISS
- chat.py (196 lines): Conversational agent with Gemini
- prompt.py (144 lines): Grounded prompt templates

Documentation:
- README.md: Quick start and overview
- SETUP.md: Detailed setup instructions
- THIS_FILE.txt: Complete project documentation

Examples:
- example_usage.py: Programmatic API examples

Configuration:
- requirements.txt: Python dependencies
- .gitignore: Git ignore patterns

DEPENDENCIES
------------

Core:
- google-generativeai: Gemini API client
- pypdf: PDF text extraction
- sentence-transformers: Embedding generation
- faiss-cpu: Vector similarity search

Support:
- numpy: Numerical operations
- tqdm: Progress bars

Total size: ~2GB (mostly models)
Installation time: ~2-5 minutes

GETTING STARTED
---------------

1. Install: pip install -r requirements.txt
2. Get API key: https://makersuite.google.com/app/apikey
3. Set key: $env:GEMINI_API_KEY='your-key'
4. Run: python main.py document.pdf
5. Ask questions!

That's it. The system handles everything else automatically.

For detailed instructions, see SETUP.md
For usage examples, see example_usage.py
For quick reference, see README.md
